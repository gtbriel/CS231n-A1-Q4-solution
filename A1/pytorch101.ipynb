{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"pytorch101.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QcJK3kXl--c3"},"source":["# Tarefa 1-1: PyTorch 101\n","\n","Antes de começar, coloque seu nome e RA no seguinte formato\n","\n",": Nome SOBRENOME, #00000000   //   e.g.) Jurandy ALMEIDA, #12345678"]},{"cell_type":"markdown","metadata":{"id":"7sA2iBcm_cPb"},"source":["**Sua Resposta:**   \n","Seu NOME, #XXXXXXXX"]},{"cell_type":"markdown","metadata":{"id":"kQndOAmiVTO3"},"source":["# Código de Configuração\n","Antes de começarmos, é precisamos executar algum código padrão para configurar nosso ambiente. Você precisará executar novamente este código de configuração sempre que iniciar o notebook.\n","\n","Primeiro, execute esta célula para carregar a extensão [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload). Isso nos permite editar arquivos de origem `.py` e reimportá-los para o notebook para uma experiência de edição e depuração perfeita."]},{"cell_type":"code","metadata":{"id":"H5PzjwH7VTO4","executionInfo":{"status":"ok","timestamp":1612801278159,"user_tz":180,"elapsed":1961,"user":{"displayName":"Jurandy Almeida","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAfAn7X3xxMoOsC5aGaq9YSsTvwjShBp7_YHuw=s64","userId":"15673677713610956963"}}},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCtoiSyVVTO8"},"source":["### Configuração do Google Colab\n","Em seguida, precisamos executar alguns comandos para configurar nosso ambiente no Google Colab. Se você estiver executando este notebook em uma máquina local, pode pular esta seção.\n","\n","Execute a seguinte célula para montar seu Google Drive. Siga o link, entre na sua conta do Google (a mesma conta que você usou para armazenar este notebook!) E copie o código de autorização na caixa de texto que aparece abaixo."]},{"cell_type":"code","metadata":{"id":"tHG0slB6VTO8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UWjXo-vXVTO_"},"source":["Agora lembre-se do caminho em seu Google Drive onde você carregou este notebook, preencha-o abaixo. Se tudo estiver funcionando corretamente, a execução da célula a seguir deve imprimir os nomes dos arquivos da atribuição:\n","\n","```\n","['a1_auxiliares.py', 'pytorch101.ipynb', 'pytorch101.py', 'rede_duas_camadas.ipynb', 'rede_duas_camadas.py', 'rnap']\n","```"]},{"cell_type":"code","metadata":{"id":"KqMvJnNHVTPA"},"source":["import os\n","\n","# TODO: Preencha o caminho do Google Drive para onde você carregou a tarefa\n","# Exemplo: se você criar uma pasta 2020FA e colocar todos os arquivos na pasta A1, então '2020FA/A1'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2020FA/A1'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None \n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ko-wLqHWVTPC"},"source":["Depois de montar com sucesso seu Google Drive e localizar o caminho para esta atribuição, execute a seguinte célula para permitir a importação dos arquivos `.py` desta atribuição. Se funcionar corretamente, deve imprimir a mensagem:\n","\n","```\n","Olá do pytorch101.py!\n","```\n","\n","bem como a última hora de edição do arquivo `pytorch101.py`."]},{"cell_type":"code","metadata":{"id":"1AoThF9eVTPD"},"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()\n","\n","from pytorch101 import ola\n","ola()\n","\n","pytorch101_path = os.path.join(GOOGLE_DRIVE_PATH, 'pytorch101.py')\n","pytorch101_edit_time = time.ctime(os.path.getmtime(pytorch101_path))\n","print('pytorch101.py última edição em %s' % pytorch101_edit_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qc83ETI1a3o9"},"source":["# Introdução\n","\n","Python 3 e [PyTorch](https://pytorch.org/) serão usados ao longo do semestre, por isso é importante estar familiarizado com eles. Este material neste notebook foi extraido de http://cs231n.github.io/python-numpy-tutorial/ e https://github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb. Este material se concentra principalmente em PyTorch.\n","\n","Este notebook irá guiá-lo por muitos dos recursos importantes do PyTorch que você precisará usar ao longo do semestre. Em algumas células e arquivos, você verá blocos de código semelhantes a este:\n","\n","```python\n","##############################################################################\n","# TODO: Escreva a equação para uma linha\n","##############################################################################\n","pass\n","##############################################################################\n","#                             FIM DO SEU CÓDIGO                              #\n","##############################################################################\n","```\n","\n","Você deve substituir a instrução `pass` por seu próprio código e deixar os blocos intactos, assim:\n","\n","```python\n","##############################################################################\n","# TODO: Instruções sobre o que você precisa fazer\n","##############################################################################\n","y = m * x + b\n","##############################################################################\n","#                             FIM DO SEU CÓDIGO                              #\n","##############################################################################\n","```\n","\n","Ao preencher o notebook, siga as seguintes regras:\n","- Não escreva ou modifique qualquer código fora dos blocos de código\n","- Não adicione ou exclua nenhuma célula do notebook. Você pode adicionar novas células para rascunhar sua resposta, mas excluí-las antes de enviar.\n","- Execute todas as células antes de enviar. Você só receberá nota pelo código que foi executado.\n","\n","Este notebook contém muitas verificações de integridade para cada linha de código que você escreve. No entanto, **passar nessas verificações de integridade não significa que seu código está correto!** Durante a avaliação, podemos executar seu código em entradas adicionais e podemos olhar seu código para ter certeza de que você seguiu as diretrizes específicas para cada implementação. Você é encorajado a escrever casos de teste adicionais para as rotinas que você deve escrever, em vez de depender apenas das verificações de integridade do notebook."]},{"cell_type":"markdown","metadata":{"id":"hQrEwOpXb9Gh"},"source":["# Python 3\n"]},{"cell_type":"markdown","metadata":{"id":"xAKwfCs_mK3d"},"source":["Se você não está familiarizado com o Python 3, aqui estão algumas das mudanças mais comuns do Python 2 a serem observadas."]},{"cell_type":"markdown","metadata":{"id":"zjosrOn8mOMV"},"source":["### Print é uma rotina"]},{"cell_type":"code","metadata":{"id":"O41SjFuamR7d"},"source":["print(\"Olá!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nEh1swLBmQN-"},"source":["Sem parênteses, a impressão não funcionará."]},{"cell_type":"markdown","metadata":{"id":"OgPaSNS2mVPn"},"source":["### Divisão de ponto flutuante por padrão"]},{"cell_type":"code","metadata":{"id":"SQKlRZ8KmYDl"},"source":["5 / 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DOmfK0WWmb2V"},"source":["Para fazer a divisão inteira, usamos duas barras invertidas:"]},{"cell_type":"code","metadata":{"id":"UUg1MjiPmgNX"},"source":["5 // 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zeH5501nmh7W"},"source":["### Sem xrange"]},{"cell_type":"markdown","metadata":{"id":"3wNKyyilmkMy"},"source":["O xrange do Python 2 agora está mesclado com o \"range\" para Python 3 e não há xrange no Python 3. No Python 3, range(3) não cria uma lista de 3 elementos como faria no Python 2, apenas cria um iterador mais eficiente em termos de memória.\n","\n","Consequentemente,\n","\n","xrange em Python 3: não existe\n","\n","range no Python 3: tem comportamento muito semelhante ao xrange do Python 2"]},{"cell_type":"code","metadata":{"id":"dP8Dk9PAmnQh"},"source":["for i in range(3):\n","    print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SKbKDgLmqd-"},"source":["range(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wm_VcW3VmsSD"},"source":["# Se necessário, pode fazer o seguinte para obter \n","# um comportamento semelhante ao range do Python 2:\n","print(list(range(3)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1MEmHrgBsgX4"},"source":["# PyTorch"]},{"cell_type":"markdown","metadata":{"id":"c3e_Nux0siHo"},"source":["[PyTorch](https://pytorch.org/) é um arcabouço de aprendizado de máquina de código aberto. Basicamente, o PyTorch fornece alguns recursos principais:\n","\n","- Um objeto **Tensor** multidimensional, semelhante a [numpy](https://numpy.org/), mas com aceleração de GPU.\n","- Um mecanismo de **autograd** otimizado para calcular automaticamente derivadas\n","- Uma API limpa e modular para desenvolver **modelos de aprendizado profundo**\n","\n","Usaremos o PyTorch para todas as atribuições de programação ao longo do semestre. Este notebook se concentrará na **API Tensor**, já que é a parte principal do PyTorch que usaremos nas primeiras atribuições.\n","\n","Você pode encontrar mais informações sobre o PyTorch seguindo um dos [tutoriais oficiais](https://pytorch.org/tutorials/) ou [lendo a documentação](https://pytorch.org/docs/stable/)."]},{"cell_type":"markdown","metadata":{"id":"zdiO3_y-vKQ9"},"source":["Para usar o PyTorch, primeiro precisamos importar o pacote `torch`.\n","\n","Também verificamos a versão; as tarefas deste curso usarão o PyTorch versão 1.7.0, já que esta é a versão padrão do Google Colab."]},{"cell_type":"code","metadata":{"id":"sydFm14itrqq"},"source":["import torch\n","print(torch.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HrBSx6hYu8ca"},"source":["## Conceitos básicos sobre Tensor"]},{"cell_type":"markdown","metadata":{"id":"LWagwmXuvIle"},"source":["### Criando e acessando tensores"]},{"cell_type":"markdown","metadata":{"id":"Bf_SY4RzvAh_"},"source":["Um **tensor** do `toch`  é um arranjo multidimensional de valores, todos do mesmo tipo, e é indexado por uma tupla de inteiros não negativos. O número de dimensões é o **posto** do tensor; o **shape** de um tensor é uma tupla de inteiros fornecendo o tamanho do arranjo ao longo de cada dimensão.\n","\n","Podemos inicializar um tensor do `torch` a partir de listas aninhadas do Python. Podemos acessar ou modificar elementos de um tensor do PyTorch usando colchetes.\n","\n","O acesso a um elemento de um tensor do PyTorch retorna um escalar do PyTorch; podemos converter isso para um escalar do Python usando o método `.item()`:"]},{"cell_type":"code","metadata":{"id":"IpwfVUvPu_lF"},"source":["# Cria um tensor de posto 1 a partir de uma lista do Python\n","a = torch.tensor([1, 2, 3])\n","print('Aqui está a:')\n","print(a)\n","print('type(a): ', type(a))\n","print('posto de a: ', a.dim())\n","print('a.shape: ', a.shape)\n","\n","# Acessa os elementos usando colchetes\n","print()\n","print('a[0]: ', a[0])\n","print('type(a[0]): ', type(a[0]))\n","print('type(a[0].item()): ', type(a[0].item()))\n","\n","# Altera os elementos usando colchetes\n","a[1] = 10\n","print()\n","print('a depois de alterado:')\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZq4zsnLEgXH"},"source":["O exemplo acima mostra um tensor unidimensional; podemos da mesma forma criar tensores com duas ou mais dimensões:"]},{"cell_type":"code","metadata":{"id":"7TcvHxpTFUcL"},"source":["# Cria um tensor bidimensional\n","b = torch.tensor([[1, 2, 3], [4, 5, 5]])\n","print('Aqui está b:')\n","print(b)\n","print('posto de b:', b.dim())\n","print('b.shape: ', b.shape)\n","\n","# Acessa elementos de um tensor multidimensional\n","print()\n","print('b[0, 1]:', b[0, 1])\n","print('b[1, 2]:', b[1, 2])\n","\n","# Altera elementos de um tensor multidimensional\n","b[1, 1] = 100\n","print()\n","print('b depois de alterado:')\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BBOsvh53GXa8"},"source":["Agora é a **sua vez**. No arquivo `pytorch101.py`, complete a implementação das rotinas `criar_tensor_de_exemplo`, `alterar_tensor` e `contar_elementos_do_tensor` para praticar a criação, alteração e entendimento sobre o shape dos tensores."]},{"cell_type":"code","metadata":{"id":"zjCIUzbaVTPs"},"source":["from pytorch101 import criar_tensor_de_exemplo, alterar_tensor, contar_elementos_do_tensor\n","\n","# Cria um tensor de exemplo\n","x = criar_tensor_de_exemplo()\n","print('Aqui está o tensor de exemplo:')\n","print(x)\n","\n","# Altera o tensor pela modificação de alguns elementos\n","indices = [(0, 0), (1, 0), (1, 1)]\n","valores = [4, 5, 6]\n","alterar_tensor(x, indices, valores)\n","print('\\nDepois de alterado:')\n","print(x)\n","print('\\nO shape está correto: ', x.shape == (3, 2))\n","print('x[0, 0] está correto: ', x[0, 0].item() == 4)\n","print('x[1, 0] está correto: ', x[1, 0].item() == 5)\n","print('x[1, 1] está correto: ', x[1, 1].item() == 6)\n","\n","# Verifica o número de elementos no tensor de exemplo\n","num = contar_elementos_do_tensor(x)\n","print('\\nNúmero de elementos em x: ', num)\n","print('A contagem está correta: ', num == 6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yz_VDA3IvP33"},"source":["### Construtores de Tensor"]},{"cell_type":"markdown","metadata":{"id":"BoAlslEdwV-k"},"source":["PyTorch fornece muitos métodos convenientes para construir tensores; isso evita a necessidade de usar listas do Python. Por exemplo:\n","\n","- [`torch.zeros`](https://pytorch.org/docs/1.1.0/torch.html#torch.zeros): Cria um tensor somente com zeros\n","- [`torch.ones`](https://pytorch.org/docs/1.1.0/torch.html#torch.ones): Cria um tensor somente com uns\n","- [`torch.rand`](https://pytorch.org/docs/1.1.0/torch.html#torch.rand): Cria um tensor com números aleatórios uniformes\n","\n","Você pode encontrar uma lista completa de operações de criação de tensores [na documentação](https://pytorch.org/docs/stable/torch.html#creation-ops)."]},{"cell_type":"code","metadata":{"id":"FL6DXGXzxHBA"},"source":["# Cria um tensor somente com zeros\n","a = torch.zeros(2, 3)\n","print('tensor de zeros:')\n","print(a)\n","\n","# Cria um tensor somente com uns\n","b = torch.ones(1, 2)\n","print('\\ntensor de uns:')\n","print(b)\n","\n","# Cria uma matriz identidade 3x3\n","c = torch.eye(3)\n","print('\\nmatriz identidade:')\n","print(c)\n","\n","# Tensor de valores aleatórios\n","d = torch.rand(4, 5)\n","print('\\ntensor aleatório:')\n","print(d)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y9QuvWYxMsoK"},"source":["**Sua vez**: No arquivo `pytorch101.py`, complete a implementação de `criar_tensor_de_pi` para praticar o uso de um construtor de tensor.\n","\n","Dica: [`torch.full`](https://pytorch.org/docs/stable/generated/torch.full.html#torch.full)"]},{"cell_type":"code","metadata":{"id":"N_y7Z5I0NIaA"},"source":["from pytorch101 import criar_tensor_de_pi\n","\n","x = criar_tensor_de_pi(4, 5)\n","\n","print('x é um tensor:', torch.is_tensor(x))\n","print('x tem o shape correto: ', x.shape == (4, 5))\n","print('x está preenchido com pi: ', (x == 3.14).all().item() == 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rz_hiJD33fu1"},"source":["### Tipos de dados"]},{"cell_type":"markdown","metadata":{"id":"GG1xBunZ3ixx"},"source":["Nos exemplos acima, você deve ter notado que alguns de nossos tensores continham valores de ponto flutuante, enquanto outros continham valores inteiros.\n","\n","PyTorch fornece um [grande conjunto de tipos de dados numéricos](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype) que você pode usar para construir tensores. PyTorch tenta adivinhar um tipo de dados quando você cria um tensor; funções que constroem tensores normalmente têm um argumento `dtype` que você pode usar para especificar explicitamente um tipo de dados.\n","\n","Cada tensor tem um atributo `dtype` que você pode usar para verificar seu tipo de dados:"]},{"cell_type":"code","metadata":{"id":"vREVDf_n31Qz"},"source":["# Deixa o torch escolher o tipo de dados\n","x0 = torch.tensor([1, 2])   # Lista de inteiros\n","x1 = torch.tensor([1., 2.]) # Lista de reais\n","x2 = torch.tensor([1., 2])  # Lista mista\n","print('dtype quando torch escolhe por nós:')\n","print('Lista de inteiros:', x0.dtype)\n","print('Lista de reais:', x1.dtype)\n","print('Lista mista:', x2.dtype)\n","\n","# Força um tipo de dados particular\n","y0 = torch.tensor([1, 2], dtype=torch.float32)  # 32-bit float\n","y1 = torch.tensor([1, 2], dtype=torch.int32)    # 32-bit (signed) integer\n","y2 = torch.tensor([1, 2], dtype=torch.int64)    # 64-bit (signed) integer\n","print('\\ndtype quando nós forçamos um tipo de dados:')\n","print('32-bit float: ', y0.dtype)\n","print('32-bit integer: ', y1.dtype)\n","print('64-bit integer: ', y2.dtype)\n","\n","# Outras opções de criação também usam um argumento dtype \n","z0 = torch.ones(1, 2)  # Deixa o toch escolher por nós\n","z1 = torch.ones(1, 2, dtype=torch.int16) # 16-bit (signed) integer\n","z2 = torch.ones(1, 2, dtype=torch.uint8) # 8-bit (unsigned) integer\n","print('\\ntorch.ones com diferentes dtypes')\n","print('dtype padrão:', z0.dtype)\n","print('16-bit integer:', z1.dtype)\n","print('8-bit unsigned integer:', z2.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2reBgQmx_x4"},"source":["Podemos **converter** um tensor para outro tipo de dados usando o método [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to); também existem métodos de conveniência como [`.float()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.float) e [`.long()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.long) que convertem para tipos de dados específicos:"]},{"cell_type":"code","metadata":{"id":"sAMpwGsdyHAw"},"source":["x0 = torch.eye(3, dtype=torch.int64)\n","x1 = x0.float()  # Converte para 32-bit float\n","x2 = x0.double() # Converte para 64-bit float\n","x3 = x0.to(torch.float32) # Forma alteranativa de converter para 32-bit float\n","x4 = x0.to(torch.float64) # Forma alteranativa de converter para 64-bit float\n","print('x0:', x0.dtype)\n","print('x1:', x1.dtype)\n","print('x2:', x2.dtype)\n","print('x3:', x3.dtype)\n","print('x4:', x4.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2O8Atl1wMB7"},"source":["PyTorch oferece várias maneiras de criar um tensor com o mesmo tipo de dados de outro tensor:\n","\n","- PyTorch fornece construtores de tensores como [`torch.zeros_like()`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like) que criam novos tensores com o mesmo shape e type de um determinado tensor\n","- Objetos de tensor têm métodos de instância, como [`.new_zeros()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.new_zeros) que criam tensores do mesmo tipo, mas possivelmente com formas diferentes\n","- O método de instância de tensor [`.to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) pode tomar um tensor como um argumento, caso em que ele converte para o tipo de dados do argumento."]},{"cell_type":"code","metadata":{"id":"1APDsx54xV6p"},"source":["x0 = torch.eye(3, dtype=torch.float64)  # Shape (3, 3), dtype torch.float64\n","x1 = torch.zeros_like(x0)               # Shape (3, 3), dtype torch.float64\n","x2 = x0.new_zeros(4, 5)                 # Shape (4, 5), dtype torch.float64\n","x3 = torch.ones(6, 7).to(x0)            # Shape (6, 7), dtype torch.float64)\n","print('x0 shape é %r, dtype é %r' % (x0.shape, x0.dtype))\n","print('x1 shape é %r, dtype é %r' % (x1.shape, x1.dtype))\n","print('x2 shape é %r, dtype é %r' % (x2.shape, x2.dtype))\n","print('x3 shape é %r, dtype é %r' % (x3.shape, x3.dtype))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OPuGPa0v4h_2"},"source":["**Sua vez**: No arquivo `pytorch101.py`, implemente a rotina ` multiplos_de_dez` que deve criar e retornar um tensor de dtype `torch.float64` contendo todos os múltiplos de dez em um determinado intervalo.\n","\n","Dica: [`torch.arange`](https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange)"]},{"cell_type":"code","metadata":{"id":"Qddo6C5Bgwcr"},"source":["from pytorch101 import multiplos_de_dez\n","\n","inicio = 5\n","fim = 25\n","x = multiplos_de_dez(inicio, fim)\n","print('O dtype está correto: ', x.dtype == torch.float64)\n","print('O shape está correto: ', x.shape == (2,))\n","print('Os valores estão corretos: ', x.tolist() == [10, 20])\n","\n","# Se não houver múltiplos de dez no intervalo dado você deve retornar um tensor vazio\n","inicio = 5\n","fim = 7\n","x = multiplos_de_dez(inicio, fim)\n","print('\\nO dtype está correto: ', x.dtype == torch.float64)\n","print('O shape está correto: ', x.shape == (0,))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RwJL3HVySvXn"},"source":["Embora o PyTorch forneça um grande número de tipos de dados numéricos, os tipos de dados mais comumente usados são:\n","\n","- `torch.float32`: Tipo de ponto flutuante padrão; usado para armazenar parâmetros aprendíveis, ativações de rede, etc. Quase toda a aritmética é feita usando este tipo.\n","- `torch.int64`: Normalmente usado para armazenar índices\n","- `torch.bool`: armazena valores booleanos: 0 é falso e 1 é verdadeiro\n","- `torch.float16`: Usado para aritmética de precisão mista, geralmente em GPUs NVIDIA com [núcleos tensores](https://www.nvidia.com/en-us/data-center/tensorcore/). Você não precisará se preocupar com este tipo de dados neste curso."]},{"cell_type":"markdown","metadata":{"id":"rlANfnILvX3S"},"source":["## Indexação de tensor"]},{"cell_type":"markdown","metadata":{"id":"KP4dRrHhyLO5"},"source":["Já vimos como obter e definir elementos individuais de tensores PyTorch. O PyTorch também fornece muitas outras maneiras de indexar em tensores. Ficar confortável com essas diferentes opções torna mais fácil modificar diferentes partes de tensores com facilidade."]},{"cell_type":"markdown","metadata":{"id":"mo-PoTWNvbba"},"source":["### Indexação de fatias"]},{"cell_type":"markdown","metadata":{"id":"qUqTYvglyVLc"},"source":["Semelhante a listas do Python e arranjos do numpy, os tensores do PyTorch podem ser **fatiados** usando a sintaxe `start:stop` ou `start:stop:step`. O índice `stop` é sempre não inclusivo: é o primeiro elemento a não ser incluído na fatia.\n","\n","Os índices `start` e `stop` podem ser negativos, caso em que contam para trás a partir do final do tensor."]},{"cell_type":"code","metadata":{"id":"yEr5BzdUdCtZ"},"source":["a = torch.tensor([0, 11, 22, 33, 44, 55, 66])\n","print(0, a)        # (0) Tensor original\n","print(1, a[2:5])   # (1) Elementos entre os índices 2 e 5\n","print(2, a[2:])    # (2) Elementos após o índice 2\n","print(3, a[:5])    # (3) Elementos antes do índice 5\n","print(4, a[:])     # (4) Todos os elementos\n","print(5, a[1:5:2]) # (5) Cada segundo elemento entre os índices 1 e 5\n","print(6, a[:-1])   # (6) Todos exceto o último elemento\n","print(7, a[-4::2]) # (7) Cada segundo elemento, começando do quarto último"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrcr9PojgTS1"},"source":["Para tensores multidimensionais, você pode fornecer uma fatia ou inteiro para cada dimensão do tensor a fim de extrair diferentes tipos de subtensores:"]},{"cell_type":"code","metadata":{"id":"S5fOdjTUyhNf"},"source":["# Cria o seguinte tensor de posto 2 com shape (3, 4)\n","# [[ 1  2  3  4]\n","#  [ 5  6  7  8]\n","#  [ 9 10 11 12]]\n","a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n","print('Tensor original:')\n","print(a)\n","print('shape: ', a.shape)\n","\n","# Acessa a linha 1 e todas as colunas.\n","print('\\nPrimeira linha:')\n","print(a[1, :])\n","print(a[1])  # Dá o mesmo resultado; podemos omitir para dimensões finais\n","print('shape: ', a[1].shape)\n","\n","print('\\nPrimeira coluna:')\n","print(a[:, 1])\n","print('shape: ', a[:, 1].shape)\n","\n","# Acessa as duas primeiras linhas e as três últimas colunas\n","print('\\nDuas primeiras linhas, duas últimas colunas:')\n","print(a[:2, -3:])\n","print('shape: ', a[:2, -3:].shape)\n","\n","# Acessa cada segunda linha e colunas nos índices 1 e 2\n","print('\\nEvery other row, middle columns:')\n","print(a[::2, 1:3])\n","print('shape: ', a[::2, 1:3].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gOsR8Pdertku"},"source":["Existem duas maneiras comuns de acessar uma única linha ou coluna de um tensor: usar um inteiro reduzirá o posto para um e usar uma fatia de comprimento um manterá o mesmo posto. Observe que este é um comportamento diferente do MATLAB."]},{"cell_type":"code","metadata":{"id":"P1kHcc5jsF-c"},"source":["# Cria o seguinte tensor de posto 2 com shape (3, 4)\n","a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n","print('Tensor original')\n","print(a)\n","\n","linha_r1 = a[1, :]    # View de posto 1 da segunda linha de a  \n","linha_r2 = a[1:2, :]  # View de posto 2 da segunda linha de a\n","print('\\nDuas maneiras de acessar uma única linha:')\n","print(linha_r1, linha_r1.shape)\n","print(linha_r2, linha_r2.shape)\n","\n","# Podemos fazer a mesma distinção ao acessar colunas:\n","coluna_r1 = a[:, 1]\n","coluna_r2 = a[:, 1:2]\n","print('\\nDuas maneiras de acessar uma única coluna:')\n","print(coluna_r1, coluna_r1.shape)\n","print(coluna_r2, coluna_r2.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jk625fJfyxV8"},"source":["O fatiamento de um tensor retorna uma **view** nos mesmos dados, portanto, modificá-lo também modificará o tensor original. Para evitar isso, você pode usar o método `clone()` para fazer uma cópia de um tensor."]},{"cell_type":"code","metadata":{"id":"IXbikYPwyxGA"},"source":["# Cria um tensor, uma fatia e uma cópia de uma fatia\n","a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n","b = a[0, 1:]\n","c = a[0, 1:].clone()\n","print('Antes de ser alterado:')\n","print(a)\n","print(b)\n","print(c)\n","\n","a[0, 1] = 20  # a[0, 1] e b[0] apontam para o mesmo elemento\n","b[1] = 30     # b[1] e a[0, 2] apontam para o mesmo elemento\n","c[2] = 40     # c é uma cópia, logo tem seus próprios dados\n","print('\\nDepois de ser alterado:')\n","print(a)\n","print(b)\n","print(c)\n","\n","print(a.storage().data_ptr() == c.storage().data_ptr())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5t5omyKwm9dB"},"source":["**Sua vez**: No arquivo `pytorch101.py`, implemente a rotina `pratica_de_indexacao_de_fatias` para praticar a indexação de tensores com diferentes tipos de fatias."]},{"cell_type":"code","metadata":{"id":"yKq2mswvqMmw"},"source":["# Usaremos esta rotina auxiliar para verificar seus resultados\n","def verifica(orig, atual, esperado):\n","    if not torch.is_tensor(atual):\n","        return False\n","    esperado = torch.tensor(esperado)\n","    mesmos_elementos = (atual == esperado).all().item()\n","    mesmos_dados = (orig.storage().data_ptr() == atual.storage().data_ptr())\n","    return mesmos_elementos and mesmos_dados"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-5UtVXPVTQL"},"source":["from pytorch101 import pratica_de_indexacao_de_fatias\n","\n","# Cria o seguinte tensor de posto 2 com shape (3, 5)\n","# [[ 1  2  3  4  5]\n","#  [ 6  7  8  9 10]\n","#  [11 12 13 14 15]]\n","x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 8, 10], [11, 12, 13, 14, 15]])\n","saida = pratica_de_indexacao_de_fatias(x)\n","\n","ultima_linha = saida[0]\n","print('ultima_linha:')\n","print(ultima_linha)\n","correto = verifica(x, ultima_linha, [11, 12, 13, 14, 15])\n","print('Está correto: %r\\n' % correto)\n","\n","terceira_coluna = saida[1]\n","print('terceira_coluna:')\n","print(terceira_coluna)\n","correto = verifica(x, terceira_coluna, [[3], [8], [13]])\n","print('Está correto: %r\\n' % correto)\n","\n","primeiras_duas_linhas_tres_colunas = saida[2]\n","print('primeiras_duas_linhas_tres_colunas:')\n","print(primeiras_duas_linhas_tres_colunas)\n","correto = verifica(x, primeiras_duas_linhas_tres_colunas, [[1, 2, 3], [6, 7, 8]])\n","print('Está correto: %r\\n' % correto)\n","\n","linhas_pares_colunas_impares = saida[3]\n","print('linhas_pares_colunas_impares:')\n","print(linhas_pares_colunas_impares)\n","correto = verifica(x, linhas_pares_colunas_impares, [[2, 4], [12, 14]])\n","print('Está correcto: %r\\n' % correto)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNjhLwb0xY2A"},"source":["Até agora, usamos o fatiamento para **acessar** subtensores; também podemos usar o fatiamento para **modificar** subtensores escrevendo expressões de atribuição em que o lado esquerdo é uma expressão de fatia e o lado direito é uma constante ou um tensor de shape correto:"]},{"cell_type":"code","metadata":{"id":"DFnky42Rx2I5"},"source":["a = torch.zeros(2, 4, dtype=torch.int64)\n","a[:, :2] = 1\n","a[:, 2:] = torch.tensor([[2, 3], [4, 5]])\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPVCQ5HszihV"},"source":["**Sua vez**: no arquivo `pytorch101.py`, implemente a rotina `pratica_atribuicao_de_fatias` para praticar a modificação de tensores com instruções de atribuição de fatias.\n","\n","Esta rotina deve usar operações de atribuição de fatias para modificar as primeiras quatro linhas e as primeiras seis colunas do tensor de entrada para que sejam iguais a\n","\n","$$\n","\\begin{bmatrix}\n","0 & 1 & 2 & 2 & 2 & 2 \\\\\n","0 & 1 & 2 & 2 & 2 & 2 \\\\\n","3 & 4 & 3 & 4 & 5 & 5 \\\\\n","3 & 4 & 3 & 4 & 5 & 5 \\\\\n","\\end{bmatrix}\n","$$\n","\n","Sua implementação deve obedecer ao seguinte:\n","- Você deve alterar o próprio tensor x e retorná-lo\n","- Você só deve modificar as primeiras 4 linhas e as primeiras 6 colunas; todos os outros elementos devem permanecer inalterados\n","- Você só pode alterar o tensor usando operações de atribuição de fatia, onde você\n","atribui um número inteiro a uma fatia do tensor\n","- Você deve usar <= 6 operações de fatiamento para alcançar o resultado desejado"]},{"cell_type":"code","metadata":{"id":"FzXlnFqAVTQQ"},"source":["from pytorch101 import pratica_atribuicao_de_fatias\n","\n","x = torch.zeros(5, 7, dtype=torch.int64)\n","print('Aqui está x antes de chamar a rotina pratica_atribuicao_de_fatias:')\n","print(x)\n","pratica_atribuicao_de_fatias(x)\n","print('Aqui está x depois de chamar a rotina pratica_atribuicao_de_fatias:')\n","print(x)\n","\n","esperado = [\n","    [0, 1, 2, 2, 2, 2, 0],\n","    [0, 1, 2, 2, 2, 2, 0],\n","    [3, 4, 3, 4, 5, 5, 0],\n","    [3, 4, 3, 4, 5, 5, 0],\n","    [0, 0, 0, 0, 0, 0, 0],\n","]\n","print('Está correto: ', x.tolist() == esperado)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4y93rPhGveWw"},"source":["### Indexação por tensor de inteiros"]},{"cell_type":"markdown","metadata":{"id":"GlTyhjEN0AIE"},"source":["Quando você indexa no tensor do toch usando o fatiamento, a view com o tensor resultante sempre será um subtensor do tensor original. Isso é poderoso, mas pode ser restritivo.\n","\n","Também podemos usar **arranjos de índices** para indexar tensores; isso nos permite construir novos tensores com muito mais flexibilidade do que usar fatias.\n","\n","Como exemplo, podemos usar arranjo de índice para reordenar as linhas ou colunas de um tensor:"]},{"cell_type":"code","metadata":{"id":"IXePPNkjM_SD"},"source":["# Cria o seguinte tensor de posto 2 com shape (3, 4)\n","# [[ 1  2  3  4]\n","#  [ 5  6  7  8]\n","#  [ 9 10 11 12]]\n","a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n","print('Tensor original:')\n","print(a)\n","\n","# Cria um novo tensor de shape (5, 4) pela reordenação das linhas de a:\n","# - As duas primeiras linhas são iguais à primeira linha de a\n","# - A terceira linha é igual a última linha de a\n","# - A quarta e quinta linhas são iguais à segunda linha de a\n","idx = [0, 0, 2, 1, 1]  # Arranjos de índices podem ser listas de inteiros do Python\n","print('\\nLinhas reordenadas:')\n","print(a[idx])\n","\n","# Cria um novo tensor de shape (3, 4) pela inversão das colunas de a\n","idx = torch.tensor([3, 2, 1, 0])  # Arranjos de índices podem ser tensores de int64 do toch\n","print('\\nColunas reordenadas:')\n","print(a[:, idx])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CpIBR1bCQji6"},"source":["De forma mais geral, dados arranjos de índices `idx0` e `idx1` com `N` elementos cada, `a[idx0, idx1]` é equivalente a:\n","\n","```\n","torch.tensor([\n","  a[idx0[0], idx1[0]],\n","  a[idx0[1], idx1[1]],\n","  ...,\n","  a[idx0[N - 1], idx1[N - 1]]\n","])\n","```\n","\n","(Um padrão semelhante se estende a tensores com mais de duas dimensões)\n","\n","Podemos, por exemplo, usar isso para acessar ou alterar a diagonal de um tensor:"]},{"cell_type":"code","metadata":{"id":"ocIR8R5ZSEaP"},"source":["a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","print('Tensor original:')\n","print(a)\n","\n","idx = [0, 1, 2]\n","print('\\nAcessa a diagonal:')\n","print(a[idx, idx])\n","\n","# Altera a diagonal\n","a[idx, idx] = torch.tensor([11, 22, 33])\n","print('\\nDepois de alterar a diagonal:')\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-cr-EqA0vfO"},"source":["Um truque útil com indexação por arranjos de inteiros é selecionar ou alterar um elemento de cada linha ou coluna de uma matriz:"]},{"cell_type":"code","metadata":{"id":"HWA8E8iI0x17"},"source":["# Cria um novo tensor a partir do qual iremos selecionar elementos\n","a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n","print('Tensor original:')\n","print(a)\n","\n","# Seleciona um elemento de cada linha de a:\n","# da linha 0, seleciona o elemento 1;\n","# da linha 1, seleciona o elemento 2;\n","# da linha 2, seleciona o elemento 1;\n","# da linha 3, seleciona o elemento 0\n","idx0 = torch.arange(a.shape[0])  # Maneira rápida de construir [0, 1, 2, 3]\n","idx1 = torch.tensor([1, 2, 1, 0])\n","print('\\nSeleciona um elemento de cada linha:')\n","print(a[idx0, idx1])\n","\n","# Agora altera cada um desses elementos para zero\n","a[idx0, idx1] = 0\n","print('\\nDepois de modificar um elemento de cada linha:')\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5_-WUmSVEoR"},"source":["**Sua vez**: no arquivo `pytorch101.py`, implemente as rotinas `embaralhar_colunas`, `inverter_linhas` e `pegar_um_elemento_por_coluna` para praticar o uso de indexação inteira para manipular tensores. Em cada uma dessas rotinas, sua implementação deve construir o tensor de saída **usando uma única operação de indexação na entrada**."]},{"cell_type":"code","metadata":{"id":"FX05_ov5VTQZ"},"source":["from pytorch101 import embaralhar_colunas, inverter_linhas, pegar_um_elemento_por_coluna\n","\n","# Cria um tensor de shape (4, 3):\n","# [[ 1,  2,  3],\n","#  [ 4,  5,  6],\n","#  [ 7,  8,  9],\n","#  [10, 11, 12]]\n","x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n","print('Aqui está x:')\n","print(x)\n","\n","y1 = embaralhar_colunas(x)\n","print('\\nAqui está embaralhar_colunas(x):')\n","print(y1)\n","esperado = [[1, 1, 3, 2], [4, 4, 6, 5], [7, 7, 9, 8], [10, 10, 12, 11]]\n","y1_correto = torch.is_tensor(y1) and y1.tolist() == esperado\n","print('Está correto: %r\\n' % y1_correto)\n","\n","y2 = inverter_linhas(x)\n","print('Aqui está inverter_linhas(x):')\n","print(y2)\n","esperado = [[10, 11, 12], [7, 8, 9], [4, 5, 6], [1, 2, 3]]\n","y2_correto = torch.is_tensor(y2) and y2.tolist() == esperado\n","print('Está correto: %r\\n' % y2_correto)\n","\n","y3 = pegar_um_elemento_por_coluna(x)\n","print('Aqui está pegar_um_elemento_por_coluna(x):')\n","print(y3)\n","esperado = [4, 2, 12]\n","y3_correto = torch.is_tensor(y3) and y3.tolist() == esperado\n","print('Está correto: %r' % y3_correto)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGt8ZPb_vixw"},"source":["### Indexação por tensor de booleanos"]},{"cell_type":"markdown","metadata":{"id":"6CkQaRj01xmU"},"source":["A indexação por tensor de booleanos permite selecionar elementos arbitrários de um tensor de acordo com uma máscara booleana. Frequentemente, esse tipo de indexação é usada para selecionar ou modificar os elementos de um tensor que satisfazem alguma condição.\n","\n","No PyTorch, usamos tensores de dtype `torch.bool` para manter máscaras booleanas.\n","\n","(Antes da versão 1.2.0, não havia o tipo `torch.bool`, então, em vez disso, `torch.uint8` era geralmente usado para representar dados booleanos, com 0 indicando falso e 1 indicando verdadeiro. Cuidado com isso no código PyTorch mais antigo!)"]},{"cell_type":"code","metadata":{"id":"29Zf7rb82Dkd"},"source":["a = torch.tensor([[1,2], [3, 4], [5, 6]])\n","print('Tensor original:')\n","print(a)\n","\n","# Encontra os elementos de a que são maiores que 3. A máscara tem \n","# o mesmo shape que a, onde cada elemento da máscara diz se o elemento \n","# correspondente de a é maior que três.\n","mask = (a > 3)\n","print('\\nTensor de máscara:')\n","print(mask)\n","\n","# Podemos usar a máscara para construir um tensor de posto 1 contendo \n","# os elementos de a que são selecionados pela máscara\n","print('\\nSelecionando elementos com a máscara:')\n","print(a[mask])\n","\n","# Também podemos usar máscaras booleanas para modificar tensores; \n","# por exemplo, isso altera todos os elementos <= 3 para zero:\n","a[a <= 3] = 0\n","print('\\nDepois de modificar com uma máscara:')\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LtSmmMGodrTX"},"source":["**Sua vez**: No arquivo `pytorch101.py`, implemente a rotina `contar_entradas_negativas` que conta o número de entradas negativas em um tensor do toch. Você pode fazer isso facilmente usando a indexação por tensor de booleanos. Sua implementação deve realizar apenas uma única operação de indexação no tensor de entrada."]},{"cell_type":"code","metadata":{"id":"2hkeYXN9d5xh"},"source":["from pytorch101 import contar_entradas_negativas\n","\n","# Cria alguns casos de teste\n","torch.manual_seed(598)\n","x0 = torch.tensor([[-1, -1, 0], [0, 1, 2], [3, 4, 5]])\n","x1 = torch.tensor([0, 1, 2, 3])\n","x2 = torch.randn(100, 100)\n","print('Está correto para x0: ', contar_entradas_negativas(x0) == 2)\n","print('Está correto para x1: ', contar_entradas_negativas(x1) == 0)\n","print('Está correto para x2: ', contar_entradas_negativas(x2) == 4984)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q68ZApgH31W9"},"source":["Agora implemente a rotina `cria_one_hot` que cria uma matriz de **vetores one-hot** a partir de uma lista de inteiros do Python.\n","\n","Um vetor one-hot para um inteiro $n$ é um vetor que tem um na $n$-ésima entrada e zeros em todas as outras entradas. Vetores one-hot são comumente usados para representar variáveis categóricas em modelos de aprendizado de máquina.\n","\n","Por exemplo, dada uma lista `[1, 4, 3, 2]` de inteiros, sua rotina deve produzir o tensor:\n","\n","```\n","[[0 1 0 0 0],\n"," [0 0 0 0 1],\n"," [0 0 0 1 0],\n"," [0 0 1 0 0]]\n","```\n","\n","Aqui, a primeira linha corresponde ao primeiro elemento da lista: ela tem um no índice 1 e zeros em todos os outros índices. A segunda linha corresponde ao segundo elemento da lista: tem um no índice 4 e zeros em todos os outros índices. As outras linhas seguem o mesmo padrão. A saída tem colunas suficientes para que nenhuma das linhas saia dos limites: o maior índice na entrada é 4, portanto, a matriz de saída tem 5 colunas."]},{"cell_type":"code","metadata":{"id":"jaT1kuQ37Rsq"},"source":["from pytorch101 import cria_one_hot\n","\n","def verifica_one_hot(x, y):\n","    C = y.shape[1]\n","    for i, n in enumerate(x):\n","        if n >= C: return False\n","        for j in range(C):\n","            esperado = 1.0 if j == n else 0.0\n","            if y[i, j].item() != esperado: return False\n","        return True\n","      \n","x0 = [1, 4, 3, 2]\n","y0 = cria_one_hot(x0)\n","print('Aqui está y0:')\n","print(y0)\n","print('y0 está correto: ', verifica_one_hot(x0, y0))\n","\n","x1 = [1, 3, 5, 7, 6, 2]\n","y1 = cria_one_hot(x1)\n","print('\\nAqui está y1:')\n","print(y1)\n","print('y1 está correto: ', verifica_one_hot(x1, y1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ad-xqELwyqpN"},"source":["## Operações de remodelagem"]},{"cell_type":"markdown","metadata":{"id":"Ql9_eXuU4OG8"},"source":["### View"]},{"cell_type":"markdown","metadata":{"id":"xfPb_2BY0HKw"},"source":["O PyTorch fornece muitas maneiras de manipular o shape dos tensores. O exemplo mais simples é [`.view()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.view): Isso retorna um novo tensor com o mesmo número de elementos que sua entrada, mas com um shape diferente.\n","\n","Podemos usar `.view()` para achatar matrizes em vetores e para converter vetores de posto 1 em matrizes de linha ou coluna de posto 2:"]},{"cell_type":"code","metadata":{"id":"kw-M7C_61FZK"},"source":["x0 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n","print('Tensor original:')\n","print(x0)\n","print('shape:', x0.shape)\n","\n","# Achata x0 em um vetor de posto 1 de shape (8,)\n","x1 = x0.view(8)\n","print('\\nTensor achatado:')\n","print(x1)\n","print('shape:', x1.shape)\n","\n","# Converte x1 em um \"vetor linha\" de posto 2 de shape (1, 8)\n","x2 = x1.view(1, 8)\n","print('\\nVetor linha:')\n","print(x2)\n","print('shape:', x2.shape)\n","\n","# Converte x1 em um \"vetor coluna\" de posto 2 de shape (8, 1)\n","x3 = x1.view(8, 1)\n","print('\\nVetor coluna:')\n","print(x3)\n","print('shape:', x3.shape)\n","\n","# Converte x1 em um tensor de posto 3 de shape (2, 2, 2):\n","x4 = x1.view(2, 2, 2)\n","print('\\nTensor de posto 3:')\n","print(x4)\n","print('shape:', x4.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eHsZ8BPF2PEq"},"source":["Por conveniência, chamadas para `.view()` podem incluir um único argumento -1; isso coloca elementos suficientes nessa dimensão para que a saída tenha o mesmo shape que a entrada. Isso torna mais fácil escrever algumas operações de remodelagem de uma forma que seja agnóstica ao shape do tensor:"]},{"cell_type":"code","metadata":{"id":"qNWu-R_J2qFY"},"source":["# Podemos reutilizar essas rotinas para tensores de diferentes formatos\n","def achata(x):\n","    return x.view(-1)\n","\n","def cria_vetor_linha(x):\n","    return x.view(1, -1)\n","\n","x0 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","x0_achatado = achata(x0)\n","x0_linha = cria_vetor_linha(x0)\n","print('x0:')\n","print(x0)\n","print('x0_achatado:')\n","print(x0_achatado)\n","print('x0_row:')\n","print(x0_linha)\n","\n","x1 = torch.tensor([[1, 2], [3, 4]])\n","x1_achatado = achata(x1)\n","x1_linha = cria_vetor_linha(x1)\n","print('\\nx1:')\n","print(x1)\n","print('x1_achatado:')\n","print(x1_achatado)\n","print('x1_linha:')\n","print(x1_linha)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DK-ZB5aB2NPq"},"source":["Como o próprio nome indica, um tensor retornado por `.view()` compartilha os mesmos dados da entrada, portanto, as alterações em um afetarão o outro e vice-versa:"]},{"cell_type":"code","metadata":{"id":"ebT99rUo2McN"},"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","x_achatado = x.view(-1)\n","print('x antes de modificar:')\n","print(x)\n","print('x_achatado antes de modificar:')\n","print(x_achatado)\n","\n","x[0, 0] = 10   # x[0, 0] e x_achatado[0] apontam para os mesmos dados\n","x_achatado[1] = 20 # x_achatado[1] e x[0, 1] apontam para os mesmos dados\n","\n","print('\\nx após modificado:')\n","print(x)\n","print('x_achatado após modificado:')\n","print(x_achatado)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z150qBob4Wkz"},"source":["### Troca de eixos"]},{"cell_type":"markdown","metadata":{"id":"TCMDxbyBys78"},"source":["Outra operação comum de remodelagem que você pode querer realizar é a transposição de uma matriz. Você pode se surpreender se tentar transpor uma matriz com `.view()`: A função `view()` pega os elementos em ordem de linha, então **você não pode transpor matrizes com `.view()`** .\n","\n","Em geral, você só deve usar `.view()` para adicionar novas dimensões a um tensor, ou para concatenar dimensões adjacentes de um tensor.\n","\n","Para outros tipos de operações de remodelagem, você geralmente precisa usar uma rotina que pode trocar os eixos de um tensor. A função mais simples é `.t()`, especificamente para transpor matrizes. Ela está disponível tanto como uma [função no módulo `torch`](https://pytorch.org/docs/stable/generated/torch.t.html#torch.t) e como um [método de instância de tensor]( https://pytorch.org/docs/stable/tensors.html#torch.Tensor.t):"]},{"cell_type":"code","metadata":{"id":"o_B4NuX6zQm-"},"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","print('Matriz original:')\n","print(x)\n","print('\\nTranspor com view NÃO FUNCIONA!')\n","print(x.view(3, 2))\n","print('\\nMatriz transposta:')\n","print(torch.t(x))\n","print(x.t())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RN93xo98zn0v"},"source":["Para tensores com mais de duas dimensões, podemos usar a função [`torch.transpose`](https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose) para trocar dimensões arbitrárias, ou o método [`.permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute) para permutar dimensões arbitrariamente:"]},{"cell_type":"code","metadata":{"id":"XgN7YB8YzzkA"},"source":["# Cria um tensor de shape (2, 3, 4)\n","x0 = torch.tensor([\n","     [[1,  2,  3,  4],\n","      [5,  6,  7,  8],\n","      [9, 10, 11, 12]],\n","     [[13, 14, 15, 16],\n","      [17, 18, 19, 20],\n","      [21, 22, 23, 24]]])\n","print('Tensor original:')\n","print(x0)\n","print('shape:', x0.shape)\n","\n","# Troca eixos 1 e 2; shape é (2, 4, 3)\n","x1 = x0.transpose(1, 2)\n","print('\\nTroca eixos 1 e 2:')\n","print(x1)\n","print(x1.shape)\n","\n","# Permuta eixos; o argumento (1, 2, 0) significa:\n","# - Faça a dimensão antiga 1 aparecer na dimensão 0;\n","# - Faça a dimensão antiga 2 aparecer na dimensão 1;\n","# - Faça a dimensão antiga 0 aparecer na dimensão 2;\n","# Isso resulta em um tensor de shape (3, 4, 2)\n","x2 = x0.permute(1, 2, 0)\n","print('\\nPermuta eixos')\n","print(x2)\n","print('shape:', x2.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4SJCVbf-bZ0"},"source":["### Tensores contíguos"]},{"cell_type":"markdown","metadata":{"id":"ubOOujO_-pQT"},"source":["Algumas combinações de operações de remodelagem falharão com erros incompreensíveis. As razões exatas para isso têm a ver com a maneira como os tensores e view de tensores são implementados e estão além do escopo desta tarefa. No entanto, se você estiver curioso, [esta postagem no blog de Edward Yang](http://blog.ezyang.com/2019/05/pytorch-internals/) fornece uma explicação clara do problema.\n","\n","O que você precisa saber é que normalmente pode superar esses tipos de erros chamando [`.contiguous()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous) antes de `.view()`, ou usando [`.reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) em vez de `.view( ) `."]},{"cell_type":"code","metadata":{"id":"YGC6NERq_CT9"},"source":["x0 = torch.randn(2, 3, 4)\n","\n","try:\n","  # Esta sequência de operações de remodelagem irá falhar\n","  x1 = x0.transpose(1, 2).view(8, 3)\n","except RuntimeError as e:\n","  print(type(e), e)\n","  \n","# Podemos resolver o problema usando .contiguous() ou .reshape()\n","x1 = x0.transpose(1, 2).contiguous().view(8, 3)\n","x2 = x0.transpose(1, 2).reshape(8, 3)\n","print('x1 shape: ', x1.shape)\n","print('x2 shape: ', x2.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJiiBxNE-X8g"},"source":["### **Sua vez**"]},{"cell_type":"markdown","metadata":{"id":"iOVzHiX-86Ew"},"source":["No arquivo `pytorch101.py`, implemente a rotina `pratica_remodelagem` para praticar o uso de operações de remodelagem em tensores. Dado o tensor de entrada unidimensional `x` contendo os números de 0 a 23 em ordem, ele deve ter o seguinte tensor de saída `y` de shape `(3, 8)` usando operações de remodelagem em x:\n","\n","\n","```\n","y = tensor([[ 0,  1,  2,  3, 12, 13, 14, 15],\n","            [ 4,  5,  6,  7, 16, 17, 18, 19],\n","            [ 8,  9, 10, 11, 20, 21, 22, 23]])\n","```\n","\n","Dica: você precisará criar um tensor intermediário de posto 3"]},{"cell_type":"code","metadata":{"id":"8reAZGzFVTQ3"},"source":["from pytorch101 import pratica_remodelagem\n","\n","x = torch.arange(24)\n","print('Aqui está x:')\n","print(x)\n","y = pratica_remodelagem(x)\n","print('Aqui está y:')\n","print(y)\n","\n","esperado = [\n","    [0, 1,  2,  3, 12, 13, 14, 15],\n","    [4, 5,  6,  7, 16, 17, 18, 19],\n","    [8, 9, 10, 11, 20, 21, 22, 23]]\n","print('Está correto:', y.tolist() == esperado)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgcdvD1evxTQ"},"source":["## Operações de tensor\n","Até agora, vimos como construir, acessar e remodelar tensores. Mas uma das razões mais importantes para usar tensores é para realizar cálculos! PyTorch fornece muitas operações diferentes para realizar cálculos em tensores."]},{"cell_type":"markdown","metadata":{"id":"1BCVlPHZ4_Qz"},"source":["### Operações por elemento"]},{"cell_type":"markdown","metadata":{"id":"f2wbN18E5CKI"},"source":["Funções matemáticas básicas operam elemento a elemento em tensores e estão disponíveis como sobrecargas de operador, como funções no módulo `torch` e como métodos de instância em objetos do torch; todos produzem os mesmos resultados:"]},{"cell_type":"code","metadata":{"id":"QrMkbk535KRZ"},"source":["x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n","y = torch.tensor([[5, 6, 7, 8]], dtype=torch.float32)\n","\n","# Soma elemento a elemento; todas dão o mesmo resultado\n","print('Soma elemento a elemento:')\n","print(x + y)\n","print(torch.add(x, y))\n","print(x.add(y))\n","\n","# Subtração elemento a elemento\n","print('\\nSubtração elemento a elemento:')\n","print(x - y)\n","print(torch.sub(x, y))\n","print(x.sub(y))\n","\n","# Multiplicação elemento a elemento\n","print('\\nMultiplicação elemento a elemento:')\n","print(x * y)\n","print(torch.mul(x, y))\n","print(x.mul(y))\n","\n","# Divisão  elemento a elemento\n","print('\\nDivisão  elemento a elemento')\n","print(x / y)\n","print(torch.div(x, y))\n","print(x.div(y))\n","\n","# Potenciação elemento a elemento\n","print('\\nPotenciação elemento a elemento')\n","print(x ** y)\n","print(torch.pow(x, y))\n","print(x.pow(y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A6WwPJMYlYvN"},"source":["O Torch também fornece muitas funções matemáticas padrão; elas estão disponíveis como funções no módulo `torch` e como métodos de instância em tensores:\n","\n","Você pode encontrar uma lista completa de todas as funções matemáticas disponíveis [na documentação](https://pytorch.org/docs/stable/torch.html#pointwise-ops); muitas funções no módulo `torch` têm métodos de instância correspondentes [em objetos tensor](https://pytorch.org/docs/stable/tensors.html)."]},{"cell_type":"code","metadata":{"id":"s87mjsnG58vR"},"source":["x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n","\n","print('Raiz quadrada:')\n","print(torch.sqrt(x))\n","print(x.sqrt())\n","\n","print('\\nFunções trigonométricas:')\n","print(torch.sin(x))\n","print(x.sin())\n","print(torch.cos(x))\n","print(x.cos())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yDyH9USAuyZ-"},"source":["### Operações de redução"]},{"cell_type":"markdown","metadata":{"id":"wbHP9SpZHoMO"},"source":["Até agora, vimos operações aritméticas básicas em tensores que operam elemento a elemento. Às vezes, podemos desejar realizar operações que agregam parte ou a totalidade de um tensor, como uma soma; são chamadas de operações de **redução**.\n","\n","Como as operações por elemento acima, a maioria das operações de redução estão disponíveis tanto como funções no módulo `torch` quanto como métodos de instância em objetos `tensor`.\n","\n","A operação de redução mais simples é a soma. Podemos usar a função [`.sum()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sum) (ou equivalentemente [`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html)\\) para reduzir um tensor inteiro ou para reduzir ao longo de apenas uma dimensão do tensor usando o argumento `dim`:"]},{"cell_type":"code","metadata":{"id":"LlmsYJWUE2r3"},"source":["x = torch.tensor([[1, 2, 3], \n","                  [4, 5, 6]], dtype=torch.float32)\n","print('Tensor original:')\n","print(x)\n","\n","print('\\nSoma sobre tensor inteiro:')\n","print(torch.sum(x))\n","print(x.sum())\n","\n","# Podemos somar cada linha:\n","print('\\nSoma de cada linha:')\n","print(torch.sum(x, dim=0))\n","print(x.sum(dim=0))\n","\n","# Podemos somar cada coluna:\n","print('\\nSoma de cada coluna:')\n","print(torch.sum(x, dim=1))\n","print(x.sum(dim=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DzKio_3Quz5a"},"source":["Outras operações de redução úteis incluem [`mean`](https://pytorch.org/docs/stable/torch.html#torch.mean), [`min`](https://pytorch.org/docs/stable/torch.html#torch.min), e [`max`](https://pytorch.org/docs/stable/torch.html#torch.max). Você pode encontrar uma lista completa de todas as operações de redução disponíveis [na documentação](https://pytorch.org/docs/stable/torch.html#reduction-ops).\n","\n","Algumas operações de redução retornam mais de um valor; por exemplo, `min` retorna o valor mínimo sobre a dimensão especificada, bem como o índice onde o valor mínimo ocorre:"]},{"cell_type":"code","metadata":{"id":"TFD7aT54H4ik"},"source":["x = torch.tensor([[2, 4, 3, 5], [3, 3, 5, 2]], dtype=torch.float32)\n","print('Tensor original:')\n","print(x, x.shape)\n","\n","# Encontra o mínimo global e retorna apenas um único valor\n","print('\\nMínimo global: ', x.min())\n","\n","# Calcula o mínimo ao longo de cada coluna; obtemos o valor e a localização:\n","# O mínimo da primeira coluna é 2 e ele aparece no índice 0;\n","# o mínimo da segunda coluna é 3 e aparece no índice 1; etc\n","coluna_min_vals, coluna_min_idxs = x.min(dim=0)\n","print('\\nMínimo ao longo de cada coluna:')\n","print('valores:', coluna_min_vals)\n","print('indices:', coluna_min_idxs)\n","\n","# Calcula o mínimo ao longo de cada linha; obtemos o valor e o mínimo\n","linha_min_vals, linha_min_idxs = x.min(dim=1)\n","print('\\nMínimo ao longo de cada linha:')\n","print('valores:', linha_min_vals)\n","print('indices:', linha_min_idxs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFwYRESoFr4t"},"source":["As operações de redução *reduzem* o posto dos tensores: a dimensão sobre a qual você executa a redução será removida do shape da saída. Se você passar `keepdim = True` para uma operação de redução, a dimensão especificada não será removida; o tensor de saída terá, em vez disso, o shape de 1 nessa dimensão.\n","\n","Quando você está trabalhando com tensores multidimensionais, pensar em linhas e colunas pode se tornar confuso; em vez disso, é mais útil pensar sobre o shape que resultará de cada operação. Por exemplo:"]},{"cell_type":"code","metadata":{"id":"sjcAveyJFqm7"},"source":["# Cria um tensor de shape (128, 10, 3, 64, 64)\n","x = torch.randn(128, 10, 3, 64, 64)\n","print(x.shape)\n","\n","# Calcula a média sobre a dimensão 1; shape agora é (128, 3, 64, 64)\n","x = x.mean(dim=1)\n","print(x.shape)\n","\n","# Calcula a soma sobre a dimensão 2; shape agora é (128, 3, 64)\n","x = x.sum(dim=2)\n","print(x.shape)\n","\n","# Calcula a média sobre a dimensão 1, mas evita que a dimensão seja eliminada \n","# passando keepdim = True; shape agora é (128, 1, 64)\n","x = x.mean(dim=1, keepdim=True)\n","print(x.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gXMp4tcM0Q_E"},"source":["**Sua vez**: No arquivo `pytorch101.py`, implemente a rotina `zero_linha_min` que define o valor mínimo ao longo de cada linha de um tensor para zero. Você deve usar operações de redução e indexação e não deve usar nenhum loop explícito.\n","\n","Dica: [`clone`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clone), [`argmin`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.argmin)"]},{"cell_type":"code","metadata":{"id":"aaJzt-Y62blF"},"source":["from pytorch101 import zero_linha_min\n","\n","x0 = torch.tensor([[10, 20, 30], [2, 5, 1]])\n","print('Aqui está x0:')\n","print(x0)\n","y0 = zero_linha_min(x0)\n","print('Aqui está y0:')\n","print(y0)\n","esperado = [[0, 20, 30], [2, 5, 0]]\n","y0_correto = torch.is_tensor(y0) and y0.tolist() == esperado\n","print('y0 está correto: ', y0_correto)\n","\n","x1 = torch.tensor([[2, 5, 10, -1], [1, 3, 2, 4], [5, 6, 2, 10]])\n","print('\\nAqui está x1:')\n","print(x1)\n","y1 = zero_linha_min(x1)\n","print('Aqui está y1:')\n","print(y1)\n","esperado = [[2, 5, 10, 0], [0, 3, 2, 4], [5, 6, 0, 10]]\n","y1_correto = torch.is_tensor(y1) and y1.tolist() == esperado\n","print('y1 está correto: ', y1_correto)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7DwjbapG6MM_"},"source":["### Operações matriciais\n","\n","Observe que, ao contrário do MATLAB, * é uma multiplicação elemento a elemento, não uma multiplicação de matrizes. O PyTorch fornece várias funções de álgebra linear que calculam diferentes tipos de produtos de vetor e matriz. Os mais comumente usados ​​são:\n","\n","- [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot): Calcula o produto interno de vetores\n","- [`torch.mm`](https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm): Calcula produtos matriz-matriz\n","- [`torch.mv`](https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv): Calcula produtos vetoriais de matriz\n","- [`torch.addmm`](https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm) / [`torch.addmv`](https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv): Calcula multiplicações matriz-matriz e matriz-vetor mais um viés\n","- [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm) / [`torch.baddmm`](https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm): versões em lote de `torch.mm` e `torch.addmm`, respectivamente\n","- [`torch.matmul`](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul): Produto de matriz arbitrária que executa operações diferentes dependendo do posto das entradas. Surpreendentemente, isso é semelhante a `np.dot` em numpy.\n","\n","Você pode encontrar uma lista completa dos operadores de álgebra linear disponíveis [na documentação](https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations).\n","\n","Aqui está um exemplo de uso de `torch.dot` para calcular produtos internos. Como os outros operadores matemáticos que vimos, a maioria dos operadores de álgebra linear estão disponíveis tanto como funções no módulo `torch` quanto como métodos de instância de tensores:"]},{"cell_type":"code","metadata":{"id":"TRUYW2as6ZCh"},"source":["v = torch.tensor([9,10], dtype=torch.float32)\n","w = torch.tensor([11, 12], dtype=torch.float32)\n","\n","# Produto interno de vetores\n","print('Produto interno:')\n","print(torch.dot(v, w))\n","print(v.dot(w))\n","\n","# dot funciona apenas para vetores -- ele dará um erro para tensores de posto > 1\n","x = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n","y = torch.tensor([[5,6],[7,8]], dtype=torch.float32)\n","try:\n","  print(x.dot(y))\n","except RuntimeError as e:\n","  print(e)\n","  \n","# Em vez disso, usamos mm para produtos de matriz-matriz:\n","print('\\nProduto de matriz-matriz:')\n","print(torch.mm(x, y))\n","print(x.mm(y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQRxK34KdHm3"},"source":["Com todos os diferentes operadores de álgebra linear que o PyTorch oferece, geralmente há mais de uma maneira de computar algo. Por exemplo, para calcular produtos de matriz-vetor, podemos usar `torch.mv`; podemos remodelar o vetor para ter posto 2 e usar `torch.mm`; ou podemos usar `torch.matmul`. Todos fornecem os mesmos resultados, mas as saídas podem ter  diferentes postos:"]},{"cell_type":"code","metadata":{"id":"qqEzcnHkdRYA"},"source":["print('Aqui está x (posto 2):')\n","print(x)\n","print('\\nAqui está v (posto 1):')\n","print(v)\n","\n","# Produto de matriz-vetor com torch.mv produz uma saída de posto 1\n","print('\\nProduto de matriz-vetor com torch.mv (saída de posto 1)')\n","print(torch.mv(x, v))\n","print(x.mv(v))\n","\n","# Podemos remodelar o vetor para ter posto 2 e usar torch.mm para realizar \n","# o produto de matriz-vetor, mas o resultado terá posto 2\n","print('\\nProduto de matriz-vetor com torch.mm (saída de posto 2)')\n","print(torch.mm(x, v.view(2, 1)))\n","print(x.mm(v.view(2, 1)))\n","\n","print('\\nProduto de matriz-vetor com torch.matmul (saída de posto 1)')\n","print(torch.matmul(x, v))\n","print(x.matmul(v))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-eqQJ5IUjtNT"},"source":["**Sua vez**: No arquivo `pytorch101.py`, implemente a função `multiplicacao_de_matriz_em_lote`. Você deve implementar dois caminhos dentro desta rotina: um que usa um loop explícito sobre a dimensão do lote e outro que executa a multiplicação da matriz em lote usando uma única operação do PyTorch e nenhum loop explícito.\n","\n","Dica: [`torch.stack`](https://pytorch.org/docs/master/generated/torch.stack.html), [`bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm)"]},{"cell_type":"code","metadata":{"id":"sZD1VQHKVTRQ"},"source":["from pytorch101 import multiplicacao_de_matriz_em_lote\n","\n","B, N, M, P = 2, 3, 5, 4\n","x = torch.randn(B, N, M)\n","y = torch.randn(B, M, P)\n","z_esperado = torch.stack([x[0] @ y[0], x[1] @ y[1]])\n","\n","# Os dois podem não retornar exatamente o mesmo resultado; diferentes rotinas de \n","# álgebra linear frequentemente retornam resultados ligeiramente diferentes devido \n","# ao fato de que a matemática de ponto flutuante é não exata e não associativa.\n","z1 = multiplicacao_de_matriz_em_lote(x, y, usa_loop=True)\n","z1_dif = (z1 - z_esperado).abs().max().item()\n","print('z1 diferença: ', z1_dif)\n","print('z1 diferença dentro da tolerância: ', z1_dif < 1e-6)\n","\n","z2 = multiplicacao_de_matriz_em_lote(x, y, usa_loop=False)\n","z2_dif = (z2 - z_esperado).abs().max().item()\n","print('\\nz2 diferença: ', z2_dif)\n","print('z2 diferença dentro da tolerância: ', z2_dif < 1e-6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mbCVOr2sVTRR"},"source":["### Vetorização\n","\n","Em muitos casos, evitar loops explícitos do Python em seu código e, em vez disso, usar os operadores do PyTorch para lidar com o loop interno fará com que seu código seja executado muito mais rápido. Este estilo de escrever código, denominado **vetorização**, evita a sobrecarga do interpretador Python e também pode paralelizar melhor a computação (por exemplo, entre núcleos de CPU, em GPUs). Sempre que possível, você deve se esforçar para escrever código vetorizado.\n","\n","Execute o seguinte para comparar a velocidade de `multiplicacao_de_matriz_em_lote` com` usa_loop = True` e com `usa_loop = False`."]},{"cell_type":"code","metadata":{"id":"a-acTIOpVTRR"},"source":["import time\n","import matplotlib.pyplot as plt\n","from pytorch101 import multiplicacao_de_matriz_em_lote\n","\n","N, M, P = 100, 100, 100\n","tempos_com_loop = []\n","tempos_sem_loop = []\n","Bs = list(range(5, 100, 5))\n","num_repeticoes = 20\n","for B in Bs:\n","    repeticoes_com_loop = []\n","    repeticoes_sem_loop = []\n","    for rep in range(num_repeticoes):\n","        x = torch.randn(B, N, M)\n","        y = torch.randn(B, M, P)\n","        t0 = time.time()\n","        z1 = multiplicacao_de_matriz_em_lote(x, y, usa_loop=True)\n","        t1 = time.time()\n","        z2 = multiplicacao_de_matriz_em_lote(x, y, usa_loop=False)\n","        t2 = time.time()\n","        repeticoes_com_loop.append(t1 - t0)\n","        repeticoes_sem_loop.append(t2 - t1)\n","    media_com_loop = torch.tensor(repeticoes_com_loop).mean().item()\n","    media_sem_loop = torch.tensor(repeticoes_sem_loop).mean().item()\n","    tempos_com_loop.append(media_com_loop)\n","    tempos_sem_loop.append(media_sem_loop)\n","    \n","plt.plot(Bs, tempos_com_loop, 'o-', label='usa_loop=True')\n","plt.plot(Bs, tempos_sem_loop, 'o-', label='usa_loop=False')\n","plt.xlabel('Tamanho do lote B')\n","plt.ylabel('Tempo de execução (s)')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UISn2pcf9QjY"},"source":["## Transmissão"]},{"cell_type":"markdown","metadata":{"id":"fTj6f8VN9UZg"},"source":["A transmissão é um mecanismo poderoso que permite ao PyTorch trabalhar com matrizes de diferentes formatos ao realizar operações aritméticas. Frequentemente, temos um tensor menor e um tensor maior, e queremos usar o tensor menor várias vezes para realizar alguma operação no tensor maior.\n","\n","Por exemplo, suponha que desejamos adicionar um vetor constante a cada linha de um tensor. Podemos fazer assim:"]},{"cell_type":"code","metadata":{"id":"kF0Dhzlu9fef"},"source":["# Vamos adicionar o vetor v a cada linha da matriz x, \n","# armazenando o resultado na matriz y\n","x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n","v = torch.tensor([1, 0, 1])\n","y = torch.zeros_like(x)   # Cria uma matriz vazia com o mesmo shape de x\n","\n","# Adiciona o vetor v a cada linha da matriz x com um loop explícito\n","for i in range(4):\n","    y[i, :] = x[i, :] + v\n","\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gXpoBKE9vp7"},"source":["Isso funciona; entretanto, quando o tensor x é muito grande, computar um loop explícito em Python pode ser lento. Observe que adicionar o vetor v a cada linha do tensor x é equivalente a formar um tensor vv ao empilhar várias cópias de v verticalmente e, em seguida, realizar a soma de elemento a elemento de x e vv. Poderíamos implementar essa abordagem assim:"]},{"cell_type":"code","metadata":{"id":"_2_5cKeu94c2"},"source":["vv = v.repeat((4, 1))  # Empilha 4 cópias de v uma em cima do outra\n","print(vv)              # Imprime \"[[1 0 1]\n","                       #           [1 0 1]\n","                       #           [1 0 1]\n","                       #           [1 0 1]]\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1KiRj23p-QIs"},"source":["y = x + vv  # Adiciona x e vv elemento a elemento\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7NNlSsHBKib"},"source":["A transmissão do PyTorch nos permite realizar esse cálculo sem realmente criar várias cópias de v. Considere esta versão, usando a transmissão:"]},{"cell_type":"code","metadata":{"id":"2jIiZc-ABBnt"},"source":["# Vamos adicionar o vetor v a cada linha da matriz x, \n","# armazenando o resultado na matriz y\n","x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n","v = torch.tensor([1, 0, 1])\n","y = x + v  # Adiciona v a cada linha de x usando a transmissão\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuUBX8YnBSIG"},"source":["A linha y = x + v funciona mesmo que x tenha o shape (4, 3) e v tenha o shape (3,) devido à transmissão; esta linha funciona como se v realmente tivesse shape (4, 3), onde cada linha era uma cópia de v, e a soma era executada elemento a elemento.\n","\n","A transmissão de dois tensores juntos segue estas regras:\n","\n","1. Se os tensores não tiverem o mesmo posto, prefixe o shape da matriz de posto inferior com 1s até que ambas os formatos tenham o mesmo comprimento.\n","2. Os dois tensores são considerados *compatíveis* em uma dimensão se eles tiverem o mesmo tamanho na dimensão, ou se um dos tensores tiver tamanho 1 nessa dimensão.\n","3. Os tensores podem ser transmitidos juntos se forem compatíveis em todas as dimensões.\n","4. Após a transmissão, cada tensor se comporta como se tivesse shape igual ao máximo elemento a elemento dos formatos dos dois tensores de entrada.\n","5. Em qualquer dimensão onde um tensor tinha tamanho 1 e o outro tensor tinha tamanho maior que 1, o primeiro tensor se comporta como se tivesse sido copiado ao longo dessa dimensão\n","\n","Se esta explicação não fizer sentido, tente ler a explicação da [documentação](https://pytorch.org/docs/stable/notes/broadcasting.html).\n","\n","A transmissão geralmente acontece implicitamente dentro de muitos operadores do PyTorch. No entanto, também podemos transmitir explicitamente usando a função [`torch.broadcast_tensors`](https://pytorch.org/docs/stable/generated/torch.broadcast_tensors.html#torch.broadcast_tensors):"]},{"cell_type":"code","metadata":{"id":"YIlIBao3VTRc"},"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n","v = torch.tensor([1, 0, 1])\n","print('Aqui está x (antes da transmissão):')\n","print(x)\n","print('x.shape: ', x.shape)\n","print('\\nAqui está v (antes da transmissão):')\n","print(v)\n","print('v.shape: ', v.shape)\n","\n","xx, vv = torch.broadcast_tensors(x, v)\n","print('Aqui está xx (depois da transmissão):')\n","print(xx)\n","print('xx.shape: ', x.shape)\n","print('\\nAqui está vv (depois da transmissão):')\n","print(vv)\n","print('vv.shape: ', vv.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PWXtBo6eVTRf"},"source":["Observe que após a transmissão, `x` permanece o mesmo, mas `v` tem uma dimensão extra anexada ao seu shape e é duplicado para ter o mesmo shape que `x`; como eles têm o mesmo shape após a transmissão, eles podem ser adicionados aos poucos.\n","\n","Nem todas as funções suportam transmissão. Você pode encontrar funções que não oferecem suporte à transmissão dos documentos oficiais. (por exemplo, [`torch.mm`](https://pytorch.org/docs/stable/torch.html#torch.mm) não oferece suporte a transmissão, mas [`torch.matmul`](https://pytorch.org/docs/1.1.0/torch.html#torch.matmul) faz)\n","\n","A transmissão pode nos permitir implementar facilmente muitas operações diferentes. Por exemplo, podemos calcular um produto externo de vetores:"]},{"cell_type":"code","metadata":{"id":"_W-k7-hpCwlT"},"source":["# Calcula o produto externo de vetores\n","v = torch.tensor([1, 2, 3])  # v tem shape (3,)\n","w = torch.tensor([4, 5])     # w tem shape (2,)\n","# Para calcular um produto externo, primeiro remodelamos v para ser um \n","# vetor coluna de shape (3, 1); podemos então transmiti-lo contra w para \n","# produzir uma saída de shape (3, 2), que é o produto externo de v e w:\n","print(v.view(3, 1) * w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6a9EcX20moP_"},"source":["Podemos adicionar um vetor a cada linha de uma matriz:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"9bhmBiwcDF1B"},"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x tem shape (2, 3)\n","v = torch.tensor([1, 2, 3])               # v tem shape (3,)\n","print('Aqui está a matriz:')\n","print(x)\n","print('\\nAqui está o vetor:')\n","print(v)\n","\n","# x tem shape (2, 3) e v tem shape (3,) logo eles transmitem para (2, 3),\n","# resultando na seguinte matriz:\n","print('\\nAdiciona o vetor a cada linha da matriz:')\n","print(x + v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jYloJIvmm_Me"},"source":["Podemos adicionar um vetor a cada linha de uma matriz:"]},{"cell_type":"code","metadata":{"id":"TDTFKACqDK22"},"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x tem shape (2, 3)\n","w = torch.tensor([4, 5])                  # w tem shape (2,)\n","print('Aqui está a matriz:')\n","print(x)\n","print('\\nAqui está o vetor:')\n","print(w)\n","\n","# x tem shape (2, 3) e w tem shape (2,). Remodelamos w para (2, 1);\n","# então, quando adicionamos os dois, o resultado transmite para (2, 3):\n","print('\\nAdiciona o vetor a cada linha da matriz:')\n","print(x + w.view(-1, 1))\n","\n","# Outra solução é a seguinte:\n","# 1. Transponha x para que tenha shape (3, 2)\n","# 2. Como w tem shape (2,), adicionar irá transmitir para (3, 2)\n","# 3. Transponha o resultado, resultando em um shape (2, 3)\n","print((x.t() + w).t())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9717YmBBpBfr"},"source":["Multiplique um tensor por um conjunto de constantes:"]},{"cell_type":"code","metadata":{"id":"4UjWDp_XDc_-"},"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x tem shape (2, 3)\n","c = torch.tensor([1, 10, 11, 100])        # c tem shape (4)\n","print('Aqui está a matriz:')\n","print(x)\n","print('\\nAqui está o vetor:')\n","print(c)\n","\n","# Fazemos o seguinte:\n","# 1. Remodele c de (4,) para (4, 1, 1)\n","# 2. x tem shape (2, 3). Como eles têm postos diferentes, quando multiplicamos \n","#    os dois, x se comporta como se seu shape fosse (1, 2, 3)\n","# 3. O resultado da multiplicação transmitida entre tensores de shape\n","#    (4, 1, 1) e (1, 2, 3) tem shape (4, 2, 3)\n","# 4. O resultado  y tem shape (4, 2, 3), e y[i] (shape (2, 3)) é igual a\n","#    c[i] * x\n","y = c.view(-1, 1, 1) * x\n","print('\\nMultiplica x por um conjunto de constantes:')\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2EHXFBFq1ea"},"source":["**Sua vez**: No arquivo `pytorch101.py`, implemente a rotina `normaliza_colunas` que normaliza as colunas de uma matriz. Ele deve calcular a média e o desvio padrão de cada coluna, depois subtrair a média e dividir pelo desvio padrão de cada elemento da coluna.\n","\n","Exemplo:\n","```\n","x = [[ 0,  30,  600],\n","     [ 1,  10,  200],\n","     [-1,  20,  400]]\n","```\n","- A primeira coluna tem média 0 e desvio padrão 1\n","- A segunda coluna tem média 20 e desvio padrão 10\n","- A terceira coluna tem média 400 e desvio padrão 200\n","\n","Após normalizar as colunas, o resultado deve ser:\n","```\n","y = [[ 0,  1,  1],\n","     [ 1, -1, -1],\n","     [-1,  0,  0]]\n","```\n","\n","Lembre-se de que dados escalares $x_1,\\ldots,x_M$ a média $\\mu$ e o desvio padrão $\\sigma$ são dados por\n","\n","$$\\mu=\\frac{1}{M}\\sum_{i=1}^M x_i \\hspace{4pc} \\sigma = \\sqrt{\\frac{1}{M-1}\\sum_{i=1}^M(x_i-\\mu)^2}$$"]},{"cell_type":"code","metadata":{"id":"rVh1DMqMr3zl"},"source":["from pytorch101 import normaliza_colunas\n","\n","x = torch.tensor([[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]])\n","y = normaliza_colunas(x)\n","print('Aqui está x:')\n","print(x)\n","print('Aqui está y:')\n","print(y)\n","\n","x_esperado = [[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]]\n","y_esperado = [[0., 1., 1.], [1., -1., -1.], [-1., 0., 0.]]\n","y_correto = y.tolist() == y_esperado\n","x_correto = x.tolist() == x_esperado\n","print('y está correto: ', y_correto)\n","print('x está inalterado: ', x_correto)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWYoGFS57Vkp"},"source":["### Operadores out-of-place vs in-place"]},{"cell_type":"markdown","metadata":{"id":"NlJs-yN4VTRp"},"source":["A maioria dos operadores do PyTorch são classificados em uma de duas categorias:\n","- **Operadores out-of-place:** retornam um novo tensor. A maioria dos operadores do PyTorch se comporta dessa maneira.\n","- **Operadores in-place:** modifica e retorna o tensor de entrada. Métodos de instância que terminam com um sublinhado (como `add_()`) são in-place. Os operadores no namespace `torch` podem ser executados in-place usando o argumento de palavra-chave `out = `.\n","\n","Por exemplo:"]},{"cell_type":"code","metadata":{"id":"lnwGzmU9VTRp"},"source":["# A adição out-of-place cria e retorna um novo tensor sem modificar as entradas:\n","x = torch.tensor([1, 2, 3])\n","y = torch.tensor([3, 4, 5])\n","print('Adição out-of-place:')\n","print('Antes da adição:')\n","print('x: ', x)\n","print('y: ', y)\n","z = x.add(y)  # Igual a z = x + y ou z = torch.add(x, y)\n","print('\\nApós a adição (x e y inalterados):')\n","print('x: ', x)\n","print('y: ', y)\n","print('z: ', z)\n","print('z é x: ', z is x)\n","print('z é y: ', z is y)\n","\n","# A adição in-place modifica o tensor de entrada:\n","print('\\n\\nAdição in-place:')\n","print('Antes da adição:')\n","print('x: ', x)\n","print('y: ', y)\n","x.add_(y)  # Igual a x += y ou torch.add(x, y, out=x)\n","print('\\nApós a adição (x é modificado):')\n","print('x: ', x)\n","print('y: ', y)\n","print('z: ', z)\n","print('z é x: ', z is x)\n","print('z é y: ', z is y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNTk5heeVTRr"},"source":["Em geral, **você deve evitar operações in-place**, pois elas podem causar problemas ao calcular gradientes usando o autograd (que abordaremos em uma tarefa futura)."]},{"cell_type":"markdown","metadata":{"id":"uN6FfqU9wFeG"},"source":["## Executando em GPU"]},{"cell_type":"markdown","metadata":{"id":"Ds6SDTbrwOc1"},"source":["Um dos recursos mais importantes do PyTorch é que ele pode usar unidades de processamento gráfico (GPUs) para acelerar suas operações de tensor.\n","\n","Podemos verificar facilmente se o PyTorch está configurado para usar GPUs:\n","\n","Os tensores podem ser movidos para qualquer dispositivo usando o método `.to`."]},{"cell_type":"code","metadata":{"id":"_RkoFEVVKWlW"},"source":["import torch\n","\n","if torch.cuda.is_available:\n","  print('PyTorch pode usar GPUs!')\n","else:\n","  print('PyTorch não pode usar GPUs.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7i_5n_XuKr5k"},"source":["Você pode habilitar GPUs no Colab via Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU.\n","\n","Isso pode fazer com que o tempo de execução do Colab seja reiniciado, então vamos reimportar o torch na próxima célula.\n","\n","Já vimos que os tensores do PyTorch têm um atributo `dtype` especificando seu tipo de dados. Todos os tensores do PyTorch também têm um atributo `device` que especifica o dispositivo onde o tensor está armazenado - CPU ou CUDA (para GPUs NVIDA). Um tensor em um dispositivo CUDA usará automaticamente esse dispositivo para acelerar todas as suas operações.\n","\n","Assim como com tipos de dados, podemos usar o método [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to) para alterar o dispositivo de um tensor. Também podemos usar os métodos de conveniência `.cuda()` e `.cpu()` para mover tensores entre CPU e GPU."]},{"cell_type":"code","metadata":{"id":"D03s614dMCvy"},"source":["# Constrói um tensor na CPU\n","x0 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n","print('x0 device:', x0.device)\n","\n","# Move ele para a GPU usando .to()\n","x1 = x0.to('cuda')\n","print('x1 device:', x1.device)\n","\n","# Move ele para a GPU usando .cuda()\n","x2 = x0.cuda()\n","print('x2 device:', x2.device)\n","\n","# Move ele de volta para a CPU usando .to()\n","x3 = x1.to('cpu')\n","print('x3 device:', x3.device)\n","\n","# Move ele de volta para a CPU usando .cpu()\n","x4 = x2.cpu()\n","print('x4 device:', x4.device)\n","\n","# Podemos construir tensores diretamente na GPU também\n","y = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cuda')\n","print('y device / dtype:', y.device, y.dtype)\n","\n","# Chamar x.to(y) onde y é um tensor retornará uma cópia de x com o mesmo \n","# device e dtype de y\n","x5 = x0.to(y)\n","print('x5 device / dtype:', x5.device, x5.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-TDxICdOmJo"},"source":["Executar grandes operações de tensor em uma GPU pode ser **muito mais rápido** do que executar a operação equivalente na CPU.\n","\n","Aqui, comparamos a velocidade de adição de dois tensores de shape (10000, 10000) na CPU e GPU:\n","\n","(Observe que o código da GPU pode ser executado de forma assíncrona com o código da CPU, portanto, ao cronometrar a velocidade das operações na GPU, é importante usar `torch.cuda.synchronize` para sincronizar a CPU e a GPU.)"]},{"cell_type":"code","metadata":{"id":"GW14ZF-_PK7t"},"source":["import time\n","\n","a_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n","b_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n","\n","a_gpu = a_cpu.cuda()\n","b_gpu = b_cpu.cuda()\n","torch.cuda.synchronize()\n","\n","t0 = time.time()\n","c_cpu = a_cpu + b_cpu\n","t1 = time.time()\n","c_gpu = a_gpu + b_gpu\n","torch.cuda.synchronize()\n","t2 = time.time()\n","\n","# Verifica se eles calcularam a mesma coisa\n","dif = (c_gpu.cpu() - c_cpu).abs().max().item()\n","print('Diferença máxima entre c_gpu e c_cpu:', dif)\n","\n","tempo_cpu = 1000.0 * (t1 - t0)\n","tempo_gpu = 1000.0 * (t2 - t1)\n","print('Tempo CPU: %.2f ms' % tempo_cpu)\n","print('Tempo GPU: %.2f ms' % tempo_gpu)\n","print('Ganho GPU: %.2f x' % (tempo_cpu / tempo_gpu))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7HEAVPEwviYb"},"source":["Você deve ver que executar o mesmo cálculo na GPU foi mais de 10~30 vezes mais rápido do que na CPU! Devido às grandes acelerações que as GPUs oferecem, usaremos as GPUs para acelerar muito do nosso código de aprendizado de máquina a partir da próxima tarefa.\n","\n","**Sua vez**: Use a GPU para acelerar a seguinte operação de multiplicação de matrizes. Você deve ver um ganho de 5~10x usando a GPU."]},{"cell_type":"code","metadata":{"id":"uqEUdst7SAuZ"},"source":["import time\n","from pytorch101 import mm_na_cpu, mm_na_gpu\n","\n","x = torch.rand(512, 4096)\n","w = torch.rand(4096, 4096)\n","\n","t0 = time.time()\n","y0 = mm_na_cpu(x, w)\n","t1 = time.time()\n","\n","y1 = mm_na_gpu(x, w)\n","torch.cuda.synchronize()\n","t2 = time.time()\n","\n","print('y1 na CPU:', y1.device == torch.device('cpu'))\n","dif = (y0 - y1).abs().max().item()\n","print('Diferença máxima entre y0 e y1:', dif)\n","print('Diferença dentro da tolerância:', dif < 5e-2)\n","\n","tempo_cpu = 1000.0 * (t1 - t0)\n","tempo_gpu = 1000.0 * (t2 - t1)\n","print('Tempo CPU: %.2f ms' % tempo_cpu)\n","print('Tempo GPU: %.2f ms' % tempo_gpu)\n","print('Ganho GPU: %.2f x' % (tempo_cpu / tempo_gpu))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vekSz6WtpUXc"},"source":["Feito! Agora você pode ir para rede_duas_camadas.ipynb. Antes de ir, verifique se você gerou alguma célula adicional em cada arquivo ipynb (por exemplo, célula vazia após a última célula de código)."]}]}
